{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPecB+Uzr1I0dIZMDPePYWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Usool-Data-Science/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/blob/main/Feature_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX5-qkvf1DSU"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ChPRyRKBzx4r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, Concatenate\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g-LAxEl9z7Va"
      },
      "outputs": [],
      "source": [
        "# Define the convolutional layer\n",
        "conv_layer = nn.Sequential(\n",
        "    #take in an input image with 3 RGB color channels and apply 16 filters to produce 16 output feature maps. \n",
        "    #Each filter is a 3x3 matrix of weights that is convolved with the input image to extract features\n",
        "    nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "E3m0CSb82nPa"
      },
      "outputs": [],
      "source": [
        "# Define the self-attention layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        \n",
        "        # Project the inputs to query, key, and value\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "\n",
        "        # Compute the attention scores\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "\n",
        "        # Apply the attention to the value\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, channels, height, width)\n",
        "\n",
        "        # Apply the scaling factor and add to the input\n",
        "        out = self.gamma * out + x\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_1KMvLNu5ASR"
      },
      "outputs": [],
      "source": [
        "# Define the image transformer\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wKxwOt835AkZ"
      },
      "outputs": [],
      "source": [
        "# Instantiate the self-attention layer\n",
        "self_att_layer = SelfAttention(32)\n",
        "\n",
        "# Define the directory path and loop through the images\n",
        "# directory_path = '/content/drive/MyDrive/imageFusion'\n",
        "# output_path = '/content/drive/MyDrive/imageFusion/FusionOutput'\n",
        "\n",
        "images_to_fuse = ['/content/drive/MyDrive/imageFusion/Source3/lytro-01-A.jpg',\n",
        "                  '/content/drive/MyDrive/imageFusion/Source3/lytro-01-B-New.jpg',\n",
        "                  '/content/drive/MyDrive/imageFusion/Source3/lytro-01-C-New.jpg']\n",
        "\n",
        "\n",
        "def extract_feature_maps(images_to_fuse):\n",
        "  images_fused = []\n",
        "  for filename in images_to_fuse:\n",
        "      if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):\n",
        "          # Load the image\n",
        "          # image = Image.open(os.path.join(directory_path, filename))\n",
        "          image = Image.open(filename)\n",
        "          # Transform the image\n",
        "          image = transform(image).unsqueeze(0)\n",
        "          # Extract the features using the convolutional layer\n",
        "          features = conv_layer(image)\n",
        "          # Apply the self-attention layer to the features\n",
        "          spatial_features = self_att_layer(features)\n",
        "          # Save the spatial features to disk\n",
        "          # output_filename = os.path.splitext(filename)[0] + '.pt'\n",
        "          # torch.save(spatial_features, os.path.join(output_path, output_filename))\n",
        "          images_fused.append(spatial_features)\n",
        "  return(images_fused)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the U-Net architecture\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = torch.nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv5 = torch.nn.Conv2d(512, 256, 3, stride=1, padding=1)\n",
        "        self.upconv3 = torch.nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv6 = torch.nn.Conv2d(256, 128, 3, stride=1, padding=1)\n",
        "        self.upconv2 = torch.nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv7 = torch.nn.Conv2d(128, 64, 3, stride=1, padding=1)\n",
        "        self.upconv1 = torch.nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
        "        self.conv8 = torch.nn.Conv2d(64, 32, 3, stride=1, padding=1)\n",
        "\n",
        "        self.final_conv = torch.nn.Conv2d(32, 3, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = torch.nn.functional.relu(self.conv1(x))\n",
        "        x2 = torch.nn.functional.relu(self.conv2(x1))\n",
        "        x3 = torch.nn.functional.relu(self.conv3(x2))\n",
        "        x4 = torch.nn.functional.relu(self.conv4(x3))\n",
        "\n",
        "        # Decoder\n",
        "        y = torch.nn.functional.relu(self.upconv4(x4))\n",
        "        y = torch.cat([y, x3], dim=1)\n",
        "        y = torch.nn.functional.relu(self.conv5(y))\n",
        "\n",
        "        y = torch.nn.functional.relu(self.upconv3(y))\n",
        "        y = torch.cat([y, x2], dim=1)\n",
        "        y = torch.nn.functional.relu(self.conv6(y))\n",
        "\n",
        "        y = torch.nn.functional.relu(self.upconv2(y))\n",
        "        y = torch.cat([y, x1], dim=1)\n",
        "        y = torch.nn.functional.relu(self.conv7(y))\n",
        "\n",
        "        y = torch.nn.functional.relu(self.upconv1(y))\n",
        "        y = torch.cat([y, x], dim=1)\n",
        "        y = torch.nn.functional.relu(self.conv8(y))\n",
        "\n",
        "        y = torch.nn.functional.tanh(self.final_conv(y))\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "rSdxKtfQ89cd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the U-Net architecture\n",
        "class UNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.conv1 = torch.nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv3 = torch.nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
        "        self.conv4 = torch.nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = torch.nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv5 = torch.nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
        "        self.upconv3 = torch.nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv6 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
        "        self.upconv2 = torch.nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv7 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
        "        self.conv8 = torch.nn.Conv2d(64, 3, 3, stride=1, padding=1)\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.relu(self.conv1(x))\n",
        "        x2 = self.relu(self.conv2(x1))\n",
        "        x3 = self.relu(self.conv3(x2))\n",
        "        x4 = self.relu(self.conv4(x3))\n",
        "\n",
        "        # Decoder\n",
        "        x = self.relu(self.upconv4(x4))\n",
        "        x = torch.cat((x, x3), dim=1)\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.relu(self.upconv3(x))\n",
        "        x = torch.cat((x, x2), dim=1)\n",
        "        x = self.relu(self.conv6(x))\n",
        "        x = self.relu(self.upconv2(x))\n",
        "        x = torch.cat((x, x1), dim=1)\n",
        "        x = self.relu(self.conv7(x))\n",
        "        x = self.conv8(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "luX0egxWKMmE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature maps\n",
        "images_to_fuse = ['/content/drive/MyDrive/imageFusion/Source3/lytro-01-A.jpg',\n",
        "                  '/content/drive/MyDrive/imageFusion/Source3/lytro-01-B-New.jpg',\n",
        "                  '/content/drive/MyDrive/imageFusion/Source3/lytro-01-C-New.jpg']\n",
        "\n",
        "feature_maps = extract_feature_maps(images_to_fuse)\n",
        "\n",
        "# Concatenate the feature maps\n",
        "x = torch.cat(feature_maps, dim=1)\n",
        "\n",
        "# Instantiate the U-Net model\n",
        "model = UNet()\n",
        "\n",
        "# Generate the fused image\n",
        "fused_image = model(x)\n",
        "\n",
        "# Save the fused image\n",
        "torchvision.utils.save_image(fused_image, 'fused_image.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "qi00qqDR9An_",
        "outputId": "73833a62-4bac-4729-8e8d-9dda22bde38a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0a2251f274f0>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Generate the fused image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfused_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Save the fused image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-b80adef9cb6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 32, 3, 3], expected input[1, 96, 56, 56] to have 32 channels, but got 96 channels instead"
          ]
        }
      ]
    }
  ]
}